{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.4.1\n",
      "numpy: 1.18.2\n",
      "matplotlib: 3.2.1\n",
      "pandas: 1.0.3\n",
      "sklearn: 0.22.2.post1\n",
      "statsmodels: 0.11.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import plotly_express as px\n",
    "import plotly.io as pio\n",
    "from sklearn import preprocessing\n",
    "import pandas_profiling\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "import math\n",
    "import plotly.express as px\n",
    "import statsmodels\n",
    "import scipy\n",
    "import sklearn\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose      # for ETS Plots\n",
    "from pmdarima import auto_arima                              # for determining ARIMA orders\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.tools import diff\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# scipy\n",
    "print('scipy: %s' % scipy.__version__)\n",
    "# numpy\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "print('matplotlib: %s' % matplotlib.__version__)\n",
    "# pandas\n",
    "print('pandas: %s' % pd.__version__)\n",
    "# scikit-learn\n",
    "print('sklearn: %s' % sklearn.__version__)\n",
    "# statsmodels\n",
    "print('statsmodels: %s' % statsmodels.__version__)\n",
    "#print('unicodedata: %s' % unicodedata.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar feriados \n",
    "def isHoliday(dia,mes):\n",
    "    if dia == 1:\n",
    "        if mes == 1 or mes ==5:\n",
    "            return(True)\n",
    "    if (dia ==2 or dia==15) and mes==11:\n",
    "        return(True)\n",
    "    if (dia ==7 or dia==20) and mes == 9:\n",
    "        return(True)\n",
    "    if mes == 4 and dia==21:\n",
    "        return(True)    \n",
    "    if mes==10 and (dia==12):\n",
    "        return(True)\n",
    "    if mes==12 and (dia ==25 or dia ==24):\n",
    "        return(True)\n",
    "    if mes==12 and dia==31:\n",
    "        return(True)\n",
    "    return(False)   \n",
    "\n",
    "def AdjustFeriado(x):\n",
    "    #print(x)\n",
    "    if x['Feriado'] == 0 or x['Feriado'] == 1:\n",
    "        return(x['Feriado'])\n",
    "       # print('else')\n",
    "       # print(x['Feriado'] >=0)\n",
    "    else:\n",
    "        dia = x['judgmentDate'].day\n",
    "        mes = x['judgmentDate'].month \n",
    "        feriado = 0 if isHoliday(dia,mes) == True else 1\n",
    "        return(feriado)\n",
    "    \n",
    "def AdjustWeekDay(x):    \n",
    "    if x['weekDay'] > 0:\n",
    "        return(x['weekDay'])\n",
    "    else:\n",
    "        return(x['judgmentDate'].weekday())        \n",
    "    \n",
    "def AdjustWeekType(x):    \n",
    "    if x['weekType'] > 0:\n",
    "        return(x['weekType'])\n",
    "    else:\n",
    "        weekType = 1 if x['judgmentDate'].weekday() <5 else 0\n",
    "        return(weekType)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_teste(series,title='',prints=False):    \n",
    "    summay = ''\n",
    "    if prints:\n",
    "        print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC',) \n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():        \n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    if prints:\n",
    "        display(pd.DataFrame(out))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        summary = \"Strong evidence against the null hypothesis \\n Reject the null hypothesis \\n Data has no unit root and is stationary\"\n",
    "        if prints:\n",
    "            print(\"Strong evidence against the null hypothesis\")\n",
    "            print(\"Reject the null hypothesis\")\n",
    "            print(\"Data has no unit root and is stationary\")        \n",
    "    else:\n",
    "        summary = \"Weak evidence against the null hypothesis \\n Fail to reject the null hypothesis \\n Data has a unit root and is non-stationary\"\n",
    "        if prints:\n",
    "            print(\"Weak evidence against the null hypothesis\")\n",
    "            print(\"Fail to reject the null hypothesis\")\n",
    "            print(\"Data has a unit root and is non-stationary\")\n",
    "            \n",
    "    \n",
    "    return(result[1],title,result[0],result[2],result[3],result[4],result[5],summary) \n",
    "\n",
    "def TooDifferent(item1,item2):\n",
    "    print(df[item1].sum(),'',df[item2].sum())\n",
    "    return (df[item1].sum()/10 >  df[item2].sum() or  df[item2].sum()/10 >  df[item1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrega\n",
    "df_count_day_type = pd.read_csv('./data_joao/df_count_day_type_modificado.csv',sep=\",\")#,parse_dates=['judgmentDate'])\n",
    "df_count_week_day_type = pd.read_csv('./data_joao/df_count_week_day_type_modificado.csv',sep=\",\")\n",
    "df_count_year_month_type = pd.read_csv('./data_joao/df_count_year_month_type_modificado.csv',sep=\",\")\n",
    "\n",
    "dfUniqueTypes = df_count_day_type['type'].unique()\n",
    "\n",
    "#cria indice\n",
    "df_count_day_type.index = pd.to_datetime(df_count_day_type['judgmentDate'])\n",
    "df_count_year_month_type.index = pd.to_datetime(df_count_year_month_type['yearMonth'])\n",
    "\n",
    "#deleta colunas que sobraram da importacao e da criacao do indice\n",
    "del df_count_day_type['Unnamed: 0']\n",
    "del df_count_week_day_type['Unnamed: 0']\n",
    "del df_count_year_month_type['Unnamed: 0']\n",
    "del df_count_day_type['judgmentDate']\n",
    "del df_count_year_month_type['yearMonth']\n",
    "\n",
    "#calcula o total do dia\n",
    "df_count_day_type['total'] = df_count_day_type.groupby(['judgmentDate'])['count'].agg('sum')\n",
    "#salva as informacoes que nao serao pivotadas\n",
    "ref = df_count_day_type[['weekDay','weekType','Feriado','total']]\n",
    "#realiza o pivot\n",
    "df_count_day_typeGRP = df_count_day_type.pivot(columns='type', values='count').fillna(0)\n",
    "\n",
    "#transforma tudo pra int (count)\n",
    "for i in df_count_day_typeGRP.columns:    \n",
    "        df_count_day_typeGRP[i] = df_count_day_typeGRP[i].astype(int)\n",
    "        \n",
    "#faz o merge com as que foram salvas antes do pivot        \n",
    "df_count_day_typeGRP = pd.merge(df_count_day_typeGRP,ref,on='judgmentDate',how='left')\n",
    "df_count_day_typeGRP = df_count_day_typeGRP.fillna(0)\n",
    "df_count_day_typeGRP = df_count_day_typeGRP.sort_values(by='judgmentDate').drop_duplicates()\n",
    "\n",
    "#remove duplicadas e nulas pra fazer a frequencia diaria\n",
    "df_count_day_typeGRP = df_count_day_typeGRP.sort_values(by='judgmentDate')\n",
    "df_count_day_typeGRP = df_count_day_typeGRP.drop_duplicates()\n",
    "df_count_day_typeGRP = df_count_day_typeGRP.dropna()\n",
    "\n",
    "\n",
    "df_count_day_typeGRP.sort_values(by='judgmentDate')\n",
    "df_count_day_typeGRP['Feriado'] = df_count_day_typeGRP['Feriado'].apply( lambda x: 0 if x==True else 1)\n",
    "df_count_day_typeGRP['weekType'] = pd.get_dummies(df_count_day_typeGRP['weekType'])\n",
    "df_count_day_typeGRP = df_count_day_typeGRP.asfreq('D') #precisa pois abaixo seto os que faltam\n",
    "\n",
    "#reseta o indice agora com todos os dias inclusive vazios e seta as informa√ßoes das datas novas (sem dados)\n",
    "df_count_day_typeGRP = df_count_day_typeGRP.reset_index()    \n",
    "df_count_day_typeGRP['Feriado'] = df_count_day_typeGRP.apply(AdjustFeriado,axis=1)\n",
    "df_count_day_typeGRP['weekDay'] = df_count_day_typeGRP.apply(AdjustWeekDay,axis=1)\n",
    "df_count_day_typeGRP['weekType'] = df_count_day_typeGRP.apply(AdjustWeekType,axis=1)\n",
    "\n",
    "#volta ao estado normal (com indice e frequencia)\n",
    "df_count_day_typeGRP.index = df_count_day_typeGRP['judgmentDate']\n",
    "del df_count_day_typeGRP['judgmentDate']\n",
    "df_count_day_typeGRP = df_count_day_typeGRP.asfreq('D')\n",
    "\n",
    "#seta para zerado as datas que nao existiam antes \n",
    "df_count_day_typeGRP = df_count_day_typeGRP.fillna(0)\n",
    "\n",
    "for i in df_count_day_typeGRP.columns:    \n",
    "        df_count_day_typeGRP[i] = df_count_day_typeGRP[i].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutli = []\n",
    "df = df_count_day_typeGRP.resample('QS').sum().copy()\n",
    "df = df['2012-01-01':]\n",
    "valor = df['total'].sum()# - 1619483 - 1366115 - 813246\n",
    "for i in df.columns:    \n",
    "    dfOutli.append({'nome':i,'Qtd':df[i].sum(),'Percent':round((df[i].sum()*100)/valor,2),'QtdDiasZero':df[df[i]==0][[i]].count().item(),'QtdNaoZero':df[df[i]!=0][[i]].count().item()})\n",
    "\n",
    "    \n",
    "#PEGAR SO AQUELES QUE TEM QUANTIDADE SENAO NAO ADIANTA FAZER VAR\n",
    "    \n",
    "dfValidTypes = pd.DataFrame(dfOutli).sort_values(by='Percent',ascending=False)\n",
    "# ver apenas aqueles que tem dados em pelo menos 50% ou mais dos quartis\n",
    "#display(dfValidTypes[dfValidTypes['QtdNaoZero']>12])\n",
    "\n",
    "dfListOfTypes = np.array(dfValidTypes[dfValidTypes['QtdNaoZero']>12]['nome'])\n",
    "dfListOfTypes = np.append(dfListOfTypes,'weekDay')\n",
    "dfListOfTypes = np.append(dfListOfTypes,'weekType')\n",
    "dfListOfTypes = np.append(dfListOfTypes,'Feriado')\n",
    "dfListOfTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_day_typeGRP.head(3)\n",
    "df = df_count_day_typeGRP\n",
    "\n",
    "#USAR SOMENTE SERIES QUE NAO CONTENHAM POUCOS REGISTROS \n",
    "df = df[dfListOfTypes]\n",
    "dfMonth = df.resample('MS').sum()\n",
    "dfQuarter = df.resample('QS').sum()\n",
    "dfYear = df.resample('Y').sum()\n",
    "dfAllPeriod = df.resample('30A').sum()\n",
    "figsize=(12,8)\n",
    "field = 'total'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
